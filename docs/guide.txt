Running Project Mongoose components
===================================

All parts of Mongoose require the environment variable `MONGOOSE` to point to the root folder of the checkout.

`export MONGOOSE=$HOME/mongoose`

Also include eHive in the PERL5LIB

Versioning pipeline
-------------------

Configurations are set in conf/manager.conf and conf/logger.conf . These files tell the pipeline how to manage its own data. The **Manager** object requires a SQL database of some sort to keep track of sources and how to handle them when it gets them. It also needs a big storage space to put copies of downloaded data and indices in.

`~/ensembl/ensembl-hive/scripts/init_pipeline.pl -pipeline_url mysql://user:password@db:port/version_update_hive`

To run a job individually

`~/ensembl/ensembl-hive/scripts/run_worker.pl -url mysql://user:password@db:port/version_update_hive -job_id 1`

Once it is running, the pipeline will interrogate each remote source and download a new version if it has changed. The old version will remain archived to provide a fallback. It works by there being a database record of each source and the modules required to download or parse them. Each downloaded source is then pushed through a parser which creates an index of the content tuned for xref use. The downloading and parsing of sources is decoupled from the deduction of external references.


Building the xref database
--------------------------

A particular species can be selected from the index, and each result is unpacked and translated into RDF. The resulting set of RDF files can be imported into Virtuoso as follows

`export PATH=$PATH:/usr/local/virtuoso-opensource/bin`
`export VIRTUOSO_HOME=/usr/local/virtuoso-opensource/var/lib/virtuoso/db`


# Bulk import
`isql`
`ld_dir('path/to/rdf','triples_%','http://www.ensembl.org');`
`rdf_loader_run();`

To see what has been identified for bulk loading, use `select * from DB.DBA.load_list;` before calling `rdf_loader_run()`.



Determine xref connections
--------------------------

## Installed components required
1. Java 8 with environment variables JAVA_HOME and PATH updated accordingly
2. Fuseki
3. Standalone Jena for validation of RDF
4. Working Versioning Service as above
5. Node.js and ideally npm. This is only required for visualisation of graphs

## Useful components for developers
1. Bio::EnsEMBL::Mongoose::Persistence::TriplestoreQuery - sends SPARQL queries over HTTP
2. Bio::EnsEMBL::Versioning::Broker - gives access to indexes of xref sources
3. /viewer - Node application with SPARQL client for rendering graphs, not well developed
4. /scripts/dump_all_xref_sources.pl - turns all active indexes into RDF for a single species